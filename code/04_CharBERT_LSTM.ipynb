{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2860e5d5f5dd44c4894d16081f725bdb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95c91871cfe84554bf51387b51ece1e8","IPY_MODEL_e204b7a1cdc447dbad16d75bd78e1f0b","IPY_MODEL_7445442624214e2e89f7338be9e62396"],"layout":"IPY_MODEL_5b716a656b274e8491fbff709c2804ef"}},"95c91871cfe84554bf51387b51ece1e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32518c4da2294b19a448165345ce43ee","placeholder":"​","style":"IPY_MODEL_eb2d6e98ddd34a18953f2473cb5da742","value":"Map: 100%"}},"e204b7a1cdc447dbad16d75bd78e1f0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70e7a08a8dcb44fca8bd631edf3e4b00","max":2372,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bf45162be0c472e97b328d130a77396","value":2372}},"7445442624214e2e89f7338be9e62396":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41259030c822418784c716824b0d3f8c","placeholder":"​","style":"IPY_MODEL_bb131d98d7994753b9777a8a94f18a4a","value":" 2372/2372 [00:02&lt;00:00, 1029.44 examples/s]"}},"5b716a656b274e8491fbff709c2804ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32518c4da2294b19a448165345ce43ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb2d6e98ddd34a18953f2473cb5da742":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70e7a08a8dcb44fca8bd631edf3e4b00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf45162be0c472e97b328d130a77396":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41259030c822418784c716824b0d3f8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb131d98d7994753b9777a8a94f18a4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66a3d2db99c441aebb59f5b59235640e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d22a80b96e85427cabf404a5fbbe5274","IPY_MODEL_0edecc55177e4fd6a66855203486300c","IPY_MODEL_b593a997c40f4eefbf774fae52525fcb"],"layout":"IPY_MODEL_393dfda247dc409eb0b45cd75258ed52"}},"d22a80b96e85427cabf404a5fbbe5274":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f59f4c3360649cea8ef5d4397cddfa5","placeholder":"​","style":"IPY_MODEL_9ec1a64e73c0478cb4049b60451df354","value":"Map: 100%"}},"0edecc55177e4fd6a66855203486300c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4789e7dc315e43bfb97dee58825bbee6","max":508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfd2763a5472488fb97f19bdf4e98f49","value":508}},"b593a997c40f4eefbf774fae52525fcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28886302c9ae4a9baa7bf26ae63536e8","placeholder":"​","style":"IPY_MODEL_874868be45324bef84ee9dee3f50d060","value":" 508/508 [00:00&lt;00:00, 1097.95 examples/s]"}},"393dfda247dc409eb0b45cd75258ed52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f59f4c3360649cea8ef5d4397cddfa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ec1a64e73c0478cb4049b60451df354":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4789e7dc315e43bfb97dee58825bbee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfd2763a5472488fb97f19bdf4e98f49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28886302c9ae4a9baa7bf26ae63536e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"874868be45324bef84ee9dee3f50d060":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ec53ff4ffc842528187cda86640573c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8ed2f5040fd48ae812148e8a599fb65","IPY_MODEL_ebf94ad24043411483d23df6b2ed8c5f","IPY_MODEL_6c7e28bf6b9846bc8da94584c7354b14"],"layout":"IPY_MODEL_541f484605804e8392d6c05e5a1a6ae0"}},"c8ed2f5040fd48ae812148e8a599fb65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36a62c6699e14933a197fe39452161c9","placeholder":"​","style":"IPY_MODEL_b2fc57a7a19a4598a8549d98f0dbd6e0","value":"Map: 100%"}},"ebf94ad24043411483d23df6b2ed8c5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95604b9c960e44ddb5534b9f541d1f8c","max":509,"min":0,"orientation":"horizontal","style":"IPY_MODEL_436ba3ee68ed40ee9267836150cb4fe3","value":509}},"6c7e28bf6b9846bc8da94584c7354b14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b798291867d64dd1ab6f9c46b9cb16ab","placeholder":"​","style":"IPY_MODEL_ae97575a19bc4861906e39f299e3280a","value":" 509/509 [00:00&lt;00:00, 991.40 examples/s]"}},"541f484605804e8392d6c05e5a1a6ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36a62c6699e14933a197fe39452161c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2fc57a7a19a4598a8549d98f0dbd6e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95604b9c960e44ddb5534b9f541d1f8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"436ba3ee68ed40ee9267836150cb4fe3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b798291867d64dd1ab6f9c46b9cb16ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae97575a19bc4861906e39f299e3280a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installation & Setup"],"metadata":{"id":"ED0aB9gNXrid"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"GawjZPFAXeYL","executionInfo":{"status":"ok","timestamp":1699656424129,"user_tz":480,"elapsed":17268,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"outputs":[],"source":["#this cell only needs to be run once for every new run time\n","!pip install transformers[torch,sentencepiece] accelerate torch datasets -U --quiet"]},{"cell_type":"code","source":["#important relevant modeling libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","# Code copied from https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt.\n","import transformers\n","\n","from datasets import Dataset\n","# Code copied from Jennifer and https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","from datasets import load_dataset\n","\n","from transformers import AutoModel, AutoTokenizer, AddedToken\n","# Code copied from: https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt and https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n"],"metadata":{"id":"TFe7uUYBYgjT","executionInfo":{"status":"ok","timestamp":1699656441248,"user_tz":480,"elapsed":17130,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#copy this block over for all successive model iterations\n","import pandas as pd\n","import os\n","import json\n","import numpy as np\n","import ast\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OagJgPSujJ2x","executionInfo":{"status":"ok","timestamp":1699656442092,"user_tz":480,"elapsed":871,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}},"outputId":"29ef51de-cbc3-4705-acca-2e204abec2cc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#CharBERT model\n","charbert_name = \"imvladikon/charbert-bert-wiki\"\n","charbert_model = AutoModel.from_pretrained(charbert_name)\n","charbert_tokenizer = AutoTokenizer.from_pretrained(charbert_name)"],"metadata":{"id":"dkt425ajYV72","executionInfo":{"status":"ok","timestamp":1699656444539,"user_tz":480,"elapsed":2452,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Data processing for model specifically"],"metadata":{"id":"erUSt-8OtFdH"}},{"cell_type":"code","source":["# Changing max length to 512 as 512 is the maximium for BERT (copied from: https://huggingface.co/learn/nlp-course/)\n","max_length = 512"],"metadata":{"id":"QH7KfCoXKnHV","executionInfo":{"status":"ok","timestamp":1699656444540,"user_tz":480,"elapsed":5,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to pad or truncate the key_timing data\n","def pad_or_truncate(array):\n","    if len(array) < max_length:\n","        # If the array is shorter than the desired length, pad it with zeros.\n","        # You can use np.pad to add zeros at the end of the array.\n","        return np.pad(array, (0, max_length - len(array)), 'constant', constant_values=(0.0))\n","    else:\n","        # If the array is longer than the desired length, truncate it.\n","        # You can use array slicing to keep only the first 'desired_length' elements.\n","        return array[:max_length]\n","\n","# Function to convert a string to a Python list\n","def string_to_list(input_string):\n","    return ast.literal_eval(input_string)"],"metadata":{"id":"t9caK2cajYMp","executionInfo":{"status":"ok","timestamp":1699656444540,"user_tz":480,"elapsed":4,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["######################## READ IN DATA ###################################################################################################\n","data_path = '/content/drive/My Drive/266 Assignments/266 Final Project'\n","files = os.listdir(data_path)\n","files = [x for x in files if '.csv' in x]\n","\n","filt_df = pd.read_csv(os.path.join(data_path, files[files.index('cleaned_data.csv')]))\n","\n","#do some data cleanup to ensure timing sequence is in right data format\n","filt_df['timing_sequence'] = filt_df['timing_sequence'].apply(string_to_list)\n","filt_df['timing_sequence'] = filt_df['timing_sequence'].apply(pad_or_truncate)\n"],"metadata":{"id":"PbxbeKbHjJs1","executionInfo":{"status":"ok","timestamp":1699656446881,"user_tz":480,"elapsed":2345,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["######################## IMPORT SPECIAL TOKENS ############################################################################################\n","json_file = os.path.join(data_path, \"token_map.json\")\n","with open(json_file, 'r') as json_file:\n","    charbert_token_map = json.load(json_file)\n","\n","added_tokens = [AddedToken(token) for token in charbert_token_map.values()]"],"metadata":{"id":"ufzgWBnsmPMH","executionInfo":{"status":"ok","timestamp":1699656446884,"user_tz":480,"elapsed":18,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#split the data\n","# Split the data into training, validation, and test sets\n","X = filt_df[['key_sequence', 'timing_sequence']]\n","y = filt_df['diagnosis']\n","\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# #convert series of lists to list\n","# X_train = list(X_train)\n","# X_val = list(X_val)\n","# X_test = list(X_test)\n","\n","# y_train = y_train.to_list()\n","# y_val = y_val.to_list()\n","# y_test = y_test.to_list()\n","\n"],"metadata":{"id":"RveqvyXiaWAF","executionInfo":{"status":"ok","timestamp":1699656446884,"user_tz":480,"elapsed":15,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Put data into a pandas dataframe to then load into a hugging face dataset object\n","# Code and idea copied from https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html\n","train_df = X_train\n","train_df['labels']=y_train\n","train_df.columns = ['key_seq', 'time_seq','labels']\n","\n","# Code copied from above\n","# Made a df of our validation data\n","val_df = X_val\n","val_df['labels']=y_val\n","val_df.columns = ['key_seq', 'time_seq','labels']\n","\n","test_df = X_test\n","test_df['labels']=y_test\n","test_df.columns = ['key_seq', 'time_seq','labels']"],"metadata":{"id":"YVSHsGxnsccc","executionInfo":{"status":"ok","timestamp":1699656446885,"user_tz":480,"elapsed":15,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Code copied from https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html\n","# Make a hugging face dataset object from the pandas df we just made\n","train_dataset = Dataset.from_pandas(train_df)\n","\n","# Code copied from above\n","# Make a validation Dataset\n","val_dataset = Dataset.from_pandas(val_df)\n","\n","# Code copied from above\n","# Make a test Dataset\n","test_dataset = Dataset.from_pandas(test_df)"],"metadata":{"id":"vfJr5PJ3scU8","executionInfo":{"status":"ok","timestamp":1699656446885,"user_tz":480,"elapsed":14,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Code copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt and prior code above (likely from BERT lesson notebooks/assignment)\n","# Code also copied from https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt\n","# Create a tokenize function we'll use to tokenize the key sequences in the dataset\n","def tokenize_func(a):\n","  return charbert_tokenizer(\n","    a['key_seq'],\n","    # Changing padding to max_length, copied from https://huggingface.co/learn/nlp-course/chapter2/6?fw=pt\n","    # Getting rid of padding here (copying from https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt).\n","    # It's more efficient to do padding at the batch level later on (use \"dynamic padding\") according to the above source\n","    # padding='max_length',\n","    # Trunction true should truncate to max length: Copied from https://huggingface.co/docs/transformers/main_classes/tokenizer\n","    truncation=True,\n","    # Testing a shorter max length based on https://huggingface.co/learn/nlp-course/chapter2/5?fw=pt\n","    # Getting rid of max length from here to, copying from: https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt\n","    # Reinstating max length since got an error\n","    max_length=max_length,\n","    # Getting rid of return tensors so that this code runs! Gave an erron when it was here\n","    # return_tensors='pt'\n","    )"],"metadata":{"id":"8gZO3xKzscLX","executionInfo":{"status":"ok","timestamp":1699656447086,"user_tz":480,"elapsed":213,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Code copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","# Tokenize the key sequences which will add the results to the dataset\n","tokenized_train_dataset = train_dataset.map(tokenize_func, batched=True)\n","\n","# Code copied from above\n","# Tokenize validation dataset\n","tokenized_val_dataset = val_dataset.map(tokenize_func, batched=True)\n","\n","# Code copied from above\n","# Tokenize test dataset\n","tokenized_test_dataset = test_dataset.map(tokenize_func, batched=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["2860e5d5f5dd44c4894d16081f725bdb","95c91871cfe84554bf51387b51ece1e8","e204b7a1cdc447dbad16d75bd78e1f0b","7445442624214e2e89f7338be9e62396","5b716a656b274e8491fbff709c2804ef","32518c4da2294b19a448165345ce43ee","eb2d6e98ddd34a18953f2473cb5da742","70e7a08a8dcb44fca8bd631edf3e4b00","6bf45162be0c472e97b328d130a77396","41259030c822418784c716824b0d3f8c","bb131d98d7994753b9777a8a94f18a4a","66a3d2db99c441aebb59f5b59235640e","d22a80b96e85427cabf404a5fbbe5274","0edecc55177e4fd6a66855203486300c","b593a997c40f4eefbf774fae52525fcb","393dfda247dc409eb0b45cd75258ed52","2f59f4c3360649cea8ef5d4397cddfa5","9ec1a64e73c0478cb4049b60451df354","4789e7dc315e43bfb97dee58825bbee6","cfd2763a5472488fb97f19bdf4e98f49","28886302c9ae4a9baa7bf26ae63536e8","874868be45324bef84ee9dee3f50d060","1ec53ff4ffc842528187cda86640573c","c8ed2f5040fd48ae812148e8a599fb65","ebf94ad24043411483d23df6b2ed8c5f","6c7e28bf6b9846bc8da94584c7354b14","541f484605804e8392d6c05e5a1a6ae0","36a62c6699e14933a197fe39452161c9","b2fc57a7a19a4598a8549d98f0dbd6e0","95604b9c960e44ddb5534b9f541d1f8c","436ba3ee68ed40ee9267836150cb4fe3","b798291867d64dd1ab6f9c46b9cb16ab","ae97575a19bc4861906e39f299e3280a"]},"id":"4bTDSAdYtO1A","executionInfo":{"status":"ok","timestamp":1699656450374,"user_tz":480,"elapsed":3299,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}},"outputId":"7f482644-380c-48f7-c774-bd418e3fd154"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2372 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2860e5d5f5dd44c4894d16081f725bdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/508 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a3d2db99c441aebb59f5b59235640e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/509 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec53ff4ffc842528187cda86640573c"}},"metadata":{}}]},{"cell_type":"code","source":["#make data loaders\n","batch_size = 16\n","\n","train_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size)\n","val_dataloader = DataLoader(tokenized_val_dataset, batch_size=batch_size)\n","test_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size)"],"metadata":{"id":"JUxs-3WWpaYC","executionInfo":{"status":"ok","timestamp":1699656450374,"user_tz":480,"elapsed":21,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# CharBERT Model + LSTM\n","Note on warning below: that ['classifier.bias', 'classifier.weight'] are newly initialized. From Jennifer's OH: The CharBert model we're using likely wasn't built to work with Hugging Face's loader. We're probably losing pre-training. Could try to load it in from git directly, but that will be difficult (3 year old repo) and we'd have to match their python version, etc. Can proceed with this as is, just be aware we're likely losing some (or all?) pre-training. Also skimmed: https://discuss.huggingface.co/t/is-some-weights-of-the-model-were-not-used-warning-normal-when-pre-trained-bert-only-by-mlm/5672"],"metadata":{"id":"6aJPGt_Xb_qp"}},{"cell_type":"code","source":["# Add the special tokens from charbert_token_map to the tokenizer's vocabulary\n","charbert_tokenizer.add_tokens(added_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oo9kRokyvY21","executionInfo":{"status":"ok","timestamp":1699656450375,"user_tz":480,"elapsed":19,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}},"outputId":"0eabd261-9374-4bf3-c0a5-c773b5fa0017"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Additional features input size\n","additional_features_size = max_length"],"metadata":{"id":"Th8y4b-lmzv7","executionInfo":{"status":"ok","timestamp":1699656450375,"user_tz":480,"elapsed":14,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Code copied from: https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","# Pull in a classification version of the model\n","charbert_model = AutoModelForSequenceClassification.from_pretrained(charbert_name, num_labels=2)\n","\n","# #must also add special tokens to the model\n","charbert_model.resize_token_embeddings(len(charbert_tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT6UmHg4YfEk","executionInfo":{"status":"ok","timestamp":1699656453638,"user_tz":480,"elapsed":3276,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}},"outputId":"588edfa8-b837-4516-d68b-d3b15423530f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at imvladikon/charbert-bert-wiki and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["Embedding(28996, 768, padding_idx=0)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["class CustomModel(nn.Module):\n","    def __init__(self, charbert_model, additional_features_size, num_labels=2):\n","        super(CustomModel, self).__init__()\n","        self.charbert_model = charbert_model\n","        self.additional_features_size = additional_features_size\n","\n","        # Add LSTM layers for additional features\n","        self.lstm1 = nn.LSTM(input_size=additional_features_size, hidden_size=64, batch_first=True)\n","        self.lstm2 = nn.LSTM(input_size=64, hidden_size=32, batch_first=True)\n","        self.lstm3 = nn.LSTM(input_size=32, hidden_size=16, batch_first=True)\n","\n","\n","        # Output layer for binary classification\n","        self.classifier = nn.Linear(768 + 16, num_labels)  # 768 is the size of the CharBERT embeddings\n","\n","    def forward(self, input_ids, attention_mask, additional_features):\n","        # CharBERT forward pass\n","        charbert_outputs = self.charbert_model(input_ids=input_ids, attention_mask=attention_mask)\n","        charbert_embeddings = charbert_outputs.logits # CharBERT has no last_hidden_state\n","\n","        # LSTM forward pass for additional features\n","        lstm_outputs1, _ = self.lstm1(additional_features)\n","        lstm_outputs2, _ = self.lstm2(lstm_outputs1)\n","        lstm_outputs3, _ = self.lstm3(lstm_outputs2)\n","        # last_lstm_output = lstm_outputs[:, -1, :].unsqueeze(1)\n","\n","        print(charbert_embeddings.shape)\n","        print(lstm_outputs3.shape)\n","\n","        # Concatenate CharBERT embeddings and LSTM outputs\n","        combined_features = torch.cat((charbert_embeddings, lstm_outputs3), dim=1)\n","\n","        # Classification layer\n","        logits = self.classifier(combined_features)\n","\n","        return logits"],"metadata":{"id":"LPJ9mMwomvrc","executionInfo":{"status":"ok","timestamp":1699656853585,"user_tz":480,"elapsed":169,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Instantiate the custom model\n","model = CustomModel(charbert_model, additional_features_size)"],"metadata":{"id":"Ch5mM-OZmvn-","executionInfo":{"status":"ok","timestamp":1699656855104,"user_tz":480,"elapsed":165,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["model.classifier.weight.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvYkM7HGRueg","executionInfo":{"status":"ok","timestamp":1699656855420,"user_tz":480,"elapsed":16,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}},"outputId":"daac0e5d-ef77-46a1-f17c-f11d116ac56f"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 784])"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"xgSfBO9ln91U"}},{"cell_type":"code","source":["learning_rate=2e-5\n","#Define Loss Function and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"vAC9-WHDmvjb","executionInfo":{"status":"ok","timestamp":1699656856430,"user_tz":480,"elapsed":173,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# Code copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","  # Copied from https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt:\n","  # \"... apply the correct amount of padding to the items of the dataset we want to batch together.\n","  # ... such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it\n","   #(to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs)\n","# Setting max length, and padding - copied from https://huggingface.co/docs/transformers/main_classes/data_collator\n","# Errored out, so got rid of these\n","data_collator = DataCollatorWithPadding(tokenizer=charbert_tokenizer)"],"metadata":{"id":"PfBFOXp3s6v4","executionInfo":{"status":"ok","timestamp":1699656856989,"user_tz":480,"elapsed":13,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["num_epochs = 5"],"metadata":{"id":"Kk3iUXUXtX6f","executionInfo":{"status":"ok","timestamp":1699656857411,"user_tz":480,"elapsed":14,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Code copied from: https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","# Set up the arguments we'll use for training\n","# Additional code (paramters to use in the call) copied from HuggingFaceThreeWays_2_Trainer.ipynb walkthrough\n","# and from https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer\n","\n","args = TrainingArguments(\"test-trainer\",\n","    evaluation_strategy = 'epoch',\n","    save_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=num_epochs,\n","    weight_decay=0.01,\n","    #load_best_model_at_end=True,\n","    #metric_for_best_model='f1'\n","    )"],"metadata":{"id":"oTOipy41bx7_","executionInfo":{"status":"ok","timestamp":1699656857802,"user_tz":480,"elapsed":7,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# Copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_val_dataset,\n","    data_collator=data_collator,\n","    tokenizer=charbert_tokenizer\n",")"],"metadata":{"id":"SWC1QaGCta4t","executionInfo":{"status":"ok","timestamp":1699656858569,"user_tz":480,"elapsed":4,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Lists to store training and validation losses and accuracies\n","train_losses = []\n","val_losses = []\n","val_accuracies = []"],"metadata":{"id":"6C_gVHaco1Ut","executionInfo":{"status":"ok","timestamp":1699656858799,"user_tz":480,"elapsed":6,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(num_epochs):\n","    trainer.train()\n","    train_loss = 0.0\n","\n","    # Training loop\n","    for batch in train_dataloader:\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        key_timing = batch['time_seq']\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, additional_features=key_timing)\n","\n","        loss = criterion(outputs.logits, inputs[\"labels\"])  # Modify this to match your labels\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_dataloader)\n","    train_losses.append(avg_train_loss)\n","\n","    # Evaluate the model on the validation set\n","    results = trainer.evaluate(eval_dataset=val_dataloader)\n","    val_loss = results[\"eval_loss\"]\n","    val_accuracy = results[\"eval_accuracy\"]\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_accuracy)\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2%}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"k93ymT8os2Iw","executionInfo":{"status":"error","timestamp":1699656905308,"user_tz":480,"elapsed":597,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}},"outputId":"2a8f750a-a713-4402-bab6-2a1c8e033510"},"execution_count":50,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-17436e8cf2d5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: CustomModel.forward() missing 1 required positional argument: 'additional_features'"]}]},{"cell_type":"code","source":["# You can save the trained model at the end of training\n","trainer.save_model(\"./custom_model\")\n","\n","# Evaluation on the test set\n","results = trainer.evaluate(eval_dataset=test_dataloader)\n","test_loss = results[\"eval_loss\"]\n","test_accuracy = results[\"eval_accuracy\"]\n","\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2%}\")\n"],"metadata":{"id":"O7u77pqis6p9","executionInfo":{"status":"aborted","timestamp":1699656462813,"user_tz":480,"elapsed":14,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K3Fc8SW1o1Lc","executionInfo":{"status":"aborted","timestamp":1699656462814,"user_tz":480,"elapsed":14,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","trainer.train()"],"metadata":{"id":"A-u3D3YttazA","executionInfo":{"status":"aborted","timestamp":1699656462951,"user_tz":480,"elapsed":3,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get predictions from the model for the validation dataset\n","# Code copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","predictions = trainer.predict(tokenized_val_dataset)\n","print(predictions.predictions.shape,predictions.label_ids.shape)"],"metadata":{"id":"5dhD3zc_tgpQ","executionInfo":{"status":"aborted","timestamp":1699656462951,"user_tz":480,"elapsed":3,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","# Copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","# \"As you can see, predictions is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used).\n","# Those are the logits for each element of the dataset we passed to predict() (as you saw in the previous chapter, all Transformer models return logits).\n","# To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis\"\n","# Code copied from https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt\n","charBert_pred_labels = np.argmax(predictions.predictions, axis=-1)"],"metadata":{"id":"WcSdKyxAtgku","executionInfo":{"status":"aborted","timestamp":1699656462952,"user_tz":480,"elapsed":4,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's evaluate the model (code copied from above)\n","\n","accuracy_charBERT = accuracy_score(np.array(y_val), charBert_pred_labels)\n","precision_charBERT = precision_score(np.array(y_val), charBert_pred_labels)\n","recall_charBERT = recall_score(np.array(y_val), charBert_pred_labels)\n","f1_charBERT = f1_score(np.array(y_val), charBert_pred_labels)\n","\n","print(\"Accuracy: \", accuracy_charBERT)\n","print(\"Precision: \", precision_charBERT)\n","print(\"Recall: \", recall_charBERT)\n","print(\"F1-Score: \", f1_charBERT)"],"metadata":{"id":"OqNUXSUItgf4","executionInfo":{"status":"aborted","timestamp":1699656462952,"user_tz":480,"elapsed":3,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O80XugWS9rvB","executionInfo":{"status":"aborted","timestamp":1699656462952,"user_tz":480,"elapsed":3,"user":{"displayName":"Cynthia Xu","userId":"04832706551111327404"}}},"execution_count":null,"outputs":[]}]}